Please be very careful I do not warrenty this product in any way and I have not fully tested it.

That being said it seems to work fine for me and has been a great help!

Should handle very large data sets, essentially what it does is search through a folder, find all duplications, add them to a file, and finally uses that file to clean everything up. 

Do not run 2x without cleaning or you may have issues. 

first run "node serial.js <Path>"
next run "node removeDupes.js"

It may take a while to run but you shouldn't have buffer problems or anything as far as I can tell.
